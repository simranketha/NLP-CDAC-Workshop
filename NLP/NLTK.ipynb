{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "76N_w2gv-zPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huVKE5D6-7xL",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVR42gTt_Ce2",
        "colab_type": "code",
        "outputId": "330c5408-c9c7-4286-feff-8512a0343c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "  nltk.download('punkt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6mECG-K--kv",
        "colab_type": "code",
        "outputId": "e82e6ad9-9ca7-493a-962a-4218e8529cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#WORD TOKENIZATION\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"CDAC workshop is on 5th and 6th july.\"\n",
        "print(word_tokenize(text))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CDAC', 'workshop', 'is', 'on', '5th', 'and', '6th', 'july', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXyrhGx6_K7o",
        "colab_type": "code",
        "outputId": "19ccf9d6-8528-4e55-c361-16a84ebf5396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#SENTENCE TOKENIZATION\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"CDAC workshop is on 5th and 6th july. Will get to machine learning, deep learning, NLP and many more.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CDAC workshop is on 5th and 6th july.', 'Will get to machine learning, deep learning, NLP and many more.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33kToKT9_35K",
        "colab_type": "text"
      },
      "source": [
        "# POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMHyC2KV__Hw",
        "colab_type": "code",
        "outputId": "8006d5be-69c3-48b5-9625-e4d12e041e31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgs7lu0A_dEy",
        "colab_type": "code",
        "outputId": "8eee7f86-569a-4390-d5fb-f3bc2a87a83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk import pos_tag\n",
        "text =\"CDAC workshop is on 5th and 6th july.\".split()\n",
        "print(\"After Split:\",text)\n",
        "tokens_tag = pos_tag(text)\n",
        "print(\"After Token:\",tokens_tag)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After Split: ['CDAC', 'workshop', 'is', 'on', '5th', 'and', '6th', 'july.']\n",
            "After Token: [('CDAC', 'NNP'), ('workshop', 'NN'), ('is', 'VBZ'), ('on', 'IN'), ('5th', 'CD'), ('and', 'CC'), ('6th', 'CD'), ('july.', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE3zogAH_3OA",
        "colab_type": "code",
        "outputId": "37911847-3174-4886-f75a-e0838bcc4a35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "text = \"CDAC workshop is on 5th and 6th july.\"\n",
        "lower_case = text.lower()\n",
        "print(lower_case)\n",
        "tokens = nltk.word_tokenize(lower_case)\n",
        "tags = nltk.pos_tag(tokens)\n",
        "counts = Counter( tag for word,  tag in tags)\n",
        "print(counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cdac workshop is on 5th and 6th july.\n",
            "Counter({'NN': 3, 'CD': 2, 'VBZ': 1, 'IN': 1, 'CC': 1, '.': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQqefUwoAVZ0",
        "colab_type": "text"
      },
      "source": [
        "#  Chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz0dj78PASWz",
        "colab_type": "code",
        "outputId": "66b7cb6a-5b7e-4aa8-b34e-34f3965076e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from nltk import RegexpParser\n",
        "text = \"learn machine learning and NLP.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)\n",
        "tag = nltk.pos_tag(tokens)\n",
        "print(tag)\n",
        "grammar = \"S: {<NN>*<NN>}\"\n",
        "cp  =nltk.RegexpParser(grammar)\n",
        "result = cp.parse(tag)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['learn', 'machine', 'learning', 'and', 'NLP', '.']\n",
            "[('learn', 'JJ'), ('machine', 'NN'), ('learning', 'NN'), ('and', 'CC'), ('NLP', 'NNP'), ('.', '.')]\n",
            "(S learn/JJ (S machine/NN learning/NN) and/CC NLP/NNP ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xNHb-DDPUJF",
        "colab_type": "text"
      },
      "source": [
        "CC- coordinating conjunction ,\n",
        "NNP - proper noun, singular ,\n",
        "NN - noun, singular ,\n",
        "JJ - adjective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmeZsJrnAV-j",
        "colab_type": "code",
        "outputId": "38b5aaaf-72e1-45ca-8880-249b229acffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
        "chunker = RegexpParser(patterns)\n",
        "print(\"After Regex:\",chunker)\n",
        "output = chunker.parse(tokens_tag)\n",
        "print(\"After Chunking\",output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After Regex: chunk.RegexpParser with 1 stages:\n",
            "RegexpChunkParser with 1 rules:\n",
            "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
            "After Chunking (S\n",
            "  (mychunk CDAC/NNP workshop/NN)\n",
            "  is/VBZ\n",
            "  on/IN\n",
            "  5th/CD\n",
            "  (mychunk and/CC)\n",
            "  6th/CD\n",
            "  (mychunk july./NN))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpOi7iOYBCV7",
        "colab_type": "text"
      },
      "source": [
        "# Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSxMlq_mBEBO",
        "colab_type": "code",
        "outputId": "81e49a88-fe87-42db-98d5-95cbfd422d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "ps =PorterStemmer()\n",
        "for w in e_words:\n",
        "    rootWord=ps.stem(w)\n",
        "    print(rootWord)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wait\n",
            "wait\n",
            "wait\n",
            "wait\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mREwLeDD3Pe",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2QVdvTeD319",
        "colab_type": "code",
        "outputId": "7f3f49fa-cf28-40df-e9bf-708ac5342774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer  = PorterStemmer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming for studies is studi\n",
            "Stemming for studying is studi\n",
            "Stemming for cries is cri\n",
            "Stemming for cry is cri\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwFKczEGD_7m",
        "colab_type": "code",
        "outputId": "9c68bb0f-23ea-46f7-8fc8-4abc65cd9f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrhyAOegD7qQ",
        "colab_type": "code",
        "outputId": "1bb8f8b8-0ab8-4922-a803-a3b9d06ab14c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_CiNVbRSNiV",
        "colab_type": "text"
      },
      "source": [
        "# Stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIncyoitSRXD",
        "colab_type": "code",
        "outputId": "06127c7b-943f-4c52-ef75-84a191179563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFEAnK4_D9S8",
        "colab_type": "code",
        "outputId": "64b397f3-55a5-4ece-f0a5-0bd37a4ef136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'should', 'me', 'where', 'shouldn', \"didn't\", 'his', 'only', 'them', 'how', 'wouldn', \"isn't\", 'that', 'he', 'whom', 'be', 'mightn', 'were', 'hers', 'are', 'more', 'on', 'do', 'my', 'yourselves', 'being', 'up', 'own', \"mustn't\", \"weren't\", 'have', 'm', 'myself', 'than', 'to', 'shan', 'i', 'doing', \"you'll\", 'and', 'in', 'its', 'him', 'mustn', 'won', 'or', \"it's\", 'it', \"she's\", 're', 'other', 'which', 'll', 'off', 'hadn', 'does', 'herself', 'we', 'having', 'd', 'once', 's', 'all', 'aren', 'why', \"needn't\", 't', 'can', 'above', 'itself', 'has', 'until', \"don't\", 'yourself', \"aren't\", 'yours', 'am', 'had', \"shan't\", \"wouldn't\", 'most', 'about', 'few', 'from', 'ma', \"you'd\", 'but', 'our', 'themselves', 'because', 'over', 'both', 'don', 'been', 'when', 'these', 'then', 'y', 'each', 'himself', 'was', 'haven', 'ain', 'if', 'o', 'this', 'before', \"wasn't\", 'theirs', 'too', 'of', 'during', \"you're\", 'through', 'a', \"won't\", 'same', 'an', 've', \"haven't\", 'no', 'who', 'hasn', \"you've\", 'against', \"hadn't\", 'the', 'down', 'under', 'so', 'some', 'did', 'for', 'needn', 'again', 'at', 'they', \"mightn't\", 'ourselves', 'you', 'as', 'will', \"hasn't\", 'couldn', 'here', \"couldn't\", 'ours', 'with', 'their', 'between', 'just', 'she', \"shouldn't\", 'into', 'below', 'there', 'any', 'is', 'nor', 'didn', 'what', 'after', 'weren', \"that'll\", 'doesn', 'isn', 'wasn', 'your', 'now', 'out', 'by', 'those', 'not', \"should've\", \"doesn't\", 'her', 'while', 'very', 'further', 'such'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCoqtr9SSaep",
        "colab_type": "code",
        "outputId": "111242a7-7d05-4aeb-8cf5-76ce16d61f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "text = \"CDAC workshop is on 5th and 6th july.\"\n",
        "tokenized_sent= word_tokenize(text)\n",
        "\n",
        "filtered_sent=[]\n",
        "for w in tokenized_sent:\n",
        "    if w not in stop_words:\n",
        "        filtered_sent.append(w)\n",
        "print(\"Tokenized Sentence:\",tokenized_sent)\n",
        "print(\"Filterd Sentence:\",filtered_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized Sentence: ['CDAC', 'workshop', 'is', 'on', '5th', 'and', '6th', 'july', '.']\n",
            "Filterd Sentence: ['CDAC', 'workshop', '5th', '6th', 'july', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Nyv0XHSeEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}